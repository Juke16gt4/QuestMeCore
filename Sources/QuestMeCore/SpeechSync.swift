//
//  SpeechSync.swift
//  QuestMeCore
//
//  ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«å ´æ‰€:
//      QuestMeCore/Sources/QuestMeCore/SpeechSync.swift
//
//  ğŸ¯ ç›®çš„:
//      SwiftUI ã¨é€£å‹•ã—ãŸéŸ³å£°åˆæˆå‡¦ç†ã‚’æä¾›ã™ã‚‹ã€‚
//      - èª­ã¿ä¸Šã’ä¸­ã®æ–‡å­—ã®ã¿ã‚’ 1.4 å€ã«æ‹¡å¤§ã—ã¦å¼·èª¿è¡¨ç¤ºã€‚
//      - AVSpeechSynthesizer ã«ã‚ˆã‚‹éŸ³å£°å‡ºåŠ›ã‚’ç®¡ç†ã€‚
//      - Delegate ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ç¾åœ¨ã®ç™ºéŸ³ç¯„å›²ã‚’è¿½è·¡ã€‚
//
//  ğŸ”— ä¾å­˜é–¢ä¿‚:
//      - SwiftUI
//      - Combine
//      - AVFoundation
//      - VoiceProfile (QuestMeCore ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«)
//      - EmotionType (QuestMeCore ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«)
//
//  ğŸ‘¤ ä½œæˆè€…: æ´¥æ‘ æ·³ä¸€
//  ğŸ“… ä½œæˆæ—¥: 2025å¹´11æœˆ12æ—¥
//

import SwiftUI
import Combine
import AVFoundation

#if canImport(UIKit)
import UIKit
typealias QMFont = UIFont
#elseif canImport(AppKit)
import AppKit
typealias QMFont = NSFont
#endif

final class SpeechSync: NSObject, ObservableObject, AVSpeechSynthesizerDelegate, @unchecked Sendable {
    private let voice: VoiceProfile
    private let synthesizer = AVSpeechSynthesizer()
    
    @Published var isSpeaking: Bool = false
    @Published var currentRange: NSRange? = nil   // ç¾åœ¨èª­ã¿ä¸Šã’ä¸­ã®æ–‡å­—ç¯„å›²
    
    init(voice: VoiceProfile) {
        self.voice = voice
        super.init()
        synthesizer.delegate = self
    }
    
    func speak(_ text: String, emotion: EmotionType = .neutral) {
        let utterance = AVSpeechUtterance(string: text)
        utterance.voice = AVSpeechSynthesisVoice(language: voice.speechCode)
        synthesizer.speak(utterance)
    }
    
    // MARK: - Delegate
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer,
                           willSpeakRangeOfSpeechString characterRange: NSRange,
                           utterance: AVSpeechUtterance) {
        DispatchQueue.main.async {
            self.currentRange = characterRange
        }
    }
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer,
                           didStart utterance: AVSpeechUtterance) {
        DispatchQueue.main.async {
            self.isSpeaking = true
        }
    }
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer,
                           didFinish utterance: AVSpeechUtterance) {
        DispatchQueue.main.async {
            self.isSpeaking = false
            self.currentRange = nil
        }
    }
}

// MARK: - SwiftUI View
public struct SpeechSyncView: View {
    @ObservedObject var sync: SpeechSync
    let text: String
    let emotion: EmotionType
    
    public var body: some View {
        let attributed = NSMutableAttributedString(string: text)
        
        if let range = sync.currentRange {
            let font = QMFont.systemFont(ofSize: 24) // iOSãªã‚‰UIFont, macOSãªã‚‰NSFont
            attributed.addAttribute(.font, value: font, range: range)
        }
        
        return Text(AttributedString(attributed))
            .onAppear {
                sync.speak(text, emotion: emotion)
            }
    }
}

// MARK: - SwiftUI Preview
struct SpeechSyncPreview: PreviewProvider {
    static var previews: some View {
        SpeechSyncView(
            sync: SpeechSync(voice: VoiceProfile(style: .calm, tone: .neutral)),
            text: "ã“ã‚“ã«ã¡ã¯ã€QuestMeCoreã®ãƒ†ã‚¹ãƒˆã§ã™ï¼",
            emotion: EmotionType.happy
        )
    }
}
